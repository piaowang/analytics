reader.class=io.sugo.collect.reader.file.DefaultFileReader
writer.class=io.sugo.collect.writer.console.ConsoleWriter


&#160;|reader.kafka.topics|读取的kafka主题,可以有多个，以","分割|test_topic|



parser.class=io.sugo.collect.parser.CSVParser

extractor.chain=trim



file.reader.log.dir=log/test
writer.kafka.topic=

#parser.class=io.sugo.collect.parser.GrokParser


file.reader.log.regex=.*
file.reader.batch.size=1000
file.reader.scan.timerange=120
file.reader.scan.interval=30000
file.reader.threadpool.size=4
file.reader.host=
file.reader.grok.patterns.path=src/main/resources/patterns
file.reader.grok.expr=\\[%{NOTSPACE:logtype}\\] \\[%{CUSTOM_TIMESTAMP_ISO8601:logtime;date;yyyy-MM-dd HH:mm:ss}\\] %{JSON:json_base_request}

is.dev.mode=true
multiline.pattern=

kafka.bootstrap.servers=dev223.sugo.net:9092,dev225.sugo.net:9092,dev224.sugo.net:9092
kafka.acks=1
kafka.key.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer



extractor.chain.name=trim,linefeed
